{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# start spark session\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImprovedClustering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\faruk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
            "File \u001b[1;32mc:\\Users\\faruk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mc:\\Users\\faruk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\faruk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "File \u001b[1;32mc:\\Users\\faruk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, hour, dayofweek, when\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# start spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"ImprovedClustering\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load sample data\n",
        "df_raw = spark.read.csv(\"../data/processed/sample_df.csv\", header=True, inferSchema=True)\n",
        "print(f\"Loaded {df_raw.count()} rows\")\n",
        "print(f\"Columns: {len(df_raw.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract temporal features from Start_Time\n",
        "df_temporal = df_raw.withColumn(\"hour\", hour(col(\"Start_Time\")))\n",
        "df_temporal = df_temporal.withColumn(\"day_of_week\", dayofweek(col(\"Start_Time\")))\n",
        "\n",
        "# add rush hour flag (7-9am and 4-7pm are rush hours)\n",
        "df_temporal = df_temporal.withColumn(\n",
        "    \"rush_hour\",\n",
        "    when((col(\"hour\") >= 7) & (col(\"hour\") <= 9), 1)\n",
        "    .when((col(\"hour\") >= 16) & (col(\"hour\") <= 19), 1)\n",
        "    .otherwise(0)\n",
        ")\n",
        "\n",
        "# add weekend flag (day 1=Sunday, 7=Saturday)\n",
        "df_temporal = df_temporal.withColumn(\n",
        "    \"is_weekend\",\n",
        "    when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1)\n",
        "    .otherwise(0)\n",
        ")\n",
        "\n",
        "print(\"Added temporal features: hour, day_of_week, rush_hour, is_weekend\")\n",
        "df_temporal.select(\"Start_Time\", \"hour\", \"day_of_week\", \"rush_hour\", \"is_weekend\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# top 10 features from siddhants analysis\n",
        "top_features = [\n",
        "    \"Temperature(F)\",\n",
        "    \"Wind_Chill(F)\", \n",
        "    \"Humidity(%)\",\n",
        "    \"Pressure(in)\",\n",
        "    \"Traffic_Signal\",\n",
        "    \"Visibility(mi)\",\n",
        "    \"Wind_Speed(mph)\",\n",
        "    \"Traffic_Calming\",\n",
        "    \"Precipitation(in)\",\n",
        "    \"Stop\"\n",
        "]\n",
        "\n",
        "# temporal features\n",
        "temporal_features = [\"rush_hour\", \"is_weekend\"]\n",
        "\n",
        "# weather features\n",
        "weather_features = [\"is_clear\", \"is_rainy\", \"is_foggy\", \"is_snowy\"]\n",
        "\n",
        "# road type features\n",
        "road_features = [\"is_highway\", \"is_local\"]\n",
        "\n",
        "# spatial features\n",
        "spatial_features = [\"state_encoded\", \"high_accident_state\"]\n",
        "\n",
        "# combine all features\n",
        "all_features = top_features + temporal_features + weather_features + road_features + spatial_features\n",
        "\n",
        "print(f\"Using {len(all_features)} features:\")\n",
        "print(f\"  Top 10 importance: {len(top_features)}\")\n",
        "print(f\"  Temporal: {len(temporal_features)}\")\n",
        "print(f\"  Weather: {len(weather_features)}\")\n",
        "print(f\"  Road type: {len(road_features)}\")\n",
        "print(f\"  Spatial: {len(spatial_features)}\")\n",
        "print(f\"  Total: {len(all_features)}\")\n",
        "\n",
        "# select features and severity\n",
        "df_selected = df_spatial.select(all_features + [\"Severity\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create weather condition flags\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "# get weather condition column and convert to lowercase for matching\n",
        "df_weather = df_temporal.withColumn(\"weather_lower\", lower(col(\"Weather_Condition\")))\n",
        "\n",
        "# create binary flags for different weather conditions\n",
        "df_weather = df_weather.withColumn(\n",
        "    \"is_clear\",\n",
        "    when(col(\"weather_lower\").contains(\"clear\") | col(\"weather_lower\").contains(\"fair\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "df_weather = df_weather.withColumn(\n",
        "    \"is_rainy\",\n",
        "    when(col(\"weather_lower\").contains(\"rain\") | col(\"weather_lower\").contains(\"drizzle\") | col(\"weather_lower\").contains(\"shower\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "df_weather = df_weather.withColumn(\n",
        "    \"is_foggy\",\n",
        "    when(col(\"weather_lower\").contains(\"fog\") | col(\"weather_lower\").contains(\"mist\") | col(\"weather_lower\").contains(\"haze\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "df_weather = df_weather.withColumn(\n",
        "    \"is_snowy\",\n",
        "    when(col(\"weather_lower\").contains(\"snow\") | col(\"weather_lower\").contains(\"ice\") | col(\"weather_lower\").contains(\"sleet\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# drop temporary column\n",
        "df_weather = df_weather.drop(\"weather_lower\")\n",
        "\n",
        "print(\"Added weather flags: is_clear, is_rainy, is_foggy, is_snowy\")\n",
        "df_weather.select(\"Weather_Condition\", \"is_clear\", \"is_rainy\", \"is_foggy\", \"is_snowy\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create road type flags from street names\n",
        "df_road = df_weather.withColumn(\"street_lower\", lower(col(\"Street\")))\n",
        "\n",
        "# highway flag (interstate or highway)\n",
        "df_road = df_road.withColumn(\n",
        "    \"is_highway\",\n",
        "    when(col(\"street_lower\").contains(\"i-\") | \n",
        "         col(\"street_lower\").contains(\"interstate\") | \n",
        "         col(\"street_lower\").contains(\"hwy\") | \n",
        "         col(\"street_lower\").contains(\"highway\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# local street flag\n",
        "df_road = df_road.withColumn(\n",
        "    \"is_local\",\n",
        "    when(col(\"street_lower\").contains(\" st\") | \n",
        "         col(\"street_lower\").contains(\" ave\") | \n",
        "         col(\"street_lower\").contains(\" rd\") | \n",
        "         col(\"street_lower\").contains(\" ln\") | \n",
        "         col(\"street_lower\").contains(\" dr\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# drop temporary column\n",
        "df_road = df_road.drop(\"street_lower\")\n",
        "\n",
        "print(\"Added road type flags: is_highway, is_local\")\n",
        "df_road.select(\"Street\", \"is_highway\", \"is_local\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add spatial features\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# encode state as numeric feature\n",
        "state_indexer = StringIndexer(inputCol=\"State\", outputCol=\"state_encoded\", handleInvalid=\"keep\")\n",
        "state_model = state_indexer.fit(df_road)\n",
        "df_spatial = state_model.transform(df_road)\n",
        "\n",
        "# identify high accident states (top 5 by count in sample)\n",
        "# CA, TX, FL, NY, PA are typically highest\n",
        "df_spatial = df_spatial.withColumn(\n",
        "    \"high_accident_state\",\n",
        "    when(col(\"State\").isin(\"CA\", \"TX\", \"FL\", \"NY\", \"PA\"), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "print(\"Added spatial features: state_encoded, high_accident_state\")\n",
        "df_spatial.select(\"State\", \"state_encoded\", \"high_accident_state\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assemble features into vector column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=all_features,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "df_vector = assembler.transform(df_selected)\n",
        "print(f\"Vectorized features, rows after removing nulls: {df_vector.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test different k values\n",
        "k_values = [3, 4, 5, 6]\n",
        "results = []\n",
        "\n",
        "print(\"Testing different k values...\")\n",
        "for k in k_values:\n",
        "    kmeans = KMeans().setK(k).setSeed(38)\n",
        "    model = kmeans.fit(df_vector)\n",
        "    \n",
        "    predictions = model.transform(df_vector)\n",
        "    \n",
        "    evaluator = ClusteringEvaluator()\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "    wcss = model.summary.trainingCost\n",
        "    \n",
        "    results.append({\n",
        "        'k': k,\n",
        "        'silhouette': silhouette,\n",
        "        'wcss': wcss\n",
        "    })\n",
        "    \n",
        "    print(f\"k={k}: silhouette={silhouette:.4f}, wcss={wcss:.2f}\")\n",
        "\n",
        "# find best k based on silhouette score\n",
        "best_k = max(results, key=lambda x: x['silhouette'])['k']\n",
        "print(f\"\\nBest k: {best_k}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train with best k\n",
        "print(f\"Training final model with k={best_k}\")\n",
        "kmeans_final = KMeans().setK(best_k).setSeed(38)\n",
        "model_final = kmeans_final.fit(df_vector)\n",
        "\n",
        "predictions_final = model_final.transform(df_vector)\n",
        "evaluator_final = ClusteringEvaluator()\n",
        "final_silhouette = evaluator_final.evaluate(predictions_final)\n",
        "final_wcss = model_final.summary.trainingCost\n",
        "\n",
        "print(f\"Final silhouette score: {final_silhouette:.4f}\")\n",
        "print(f\"Final WCSS: {final_wcss:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze what each cluster represents\n",
        "print(\"\\nCluster Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for cluster_id in range(best_k):\n",
        "    cluster_data = predictions_final.filter(col(\"prediction\") == cluster_id)\n",
        "    count = cluster_data.count()\n",
        "    \n",
        "    print(f\"\\nCluster {cluster_id} ({count} accidents):\")\n",
        "    \n",
        "    # get average values for each feature\n",
        "    for feature in top_features[:5]:  # show top 5 features\n",
        "        avg_val = cluster_data.agg({feature: \"avg\"}).collect()[0][0]\n",
        "        if avg_val is not None:\n",
        "            print(f\"  {feature}: {avg_val:.2f}\")\n",
        "    \n",
        "    # show rush hour percentage\n",
        "    rush_pct = cluster_data.filter(col(\"rush_hour\") == 1).count() / count * 100\n",
        "    print(f\"  Rush hour: {rush_pct:.1f}%\")\n",
        "    \n",
        "    # show severity distribution\n",
        "    severity_dist = cluster_data.groupBy(\"Severity\").count().orderBy(\"Severity\").collect()\n",
        "    severity_str = \", \".join([f\"S{row.Severity}:{row['count']}\" for row in severity_dist])\n",
        "    print(f\"  Severity: {severity_str}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert to pandas for visualization\n",
        "pandas_df = predictions_final.select(\n",
        "    \"Temperature(F)\", \n",
        "    \"Humidity(%)\", \n",
        "    \"prediction\"\n",
        ").toPandas()\n",
        "\n",
        "# scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster_id in range(best_k):\n",
        "    cluster_points = pandas_df[pandas_df[\"prediction\"] == cluster_id]\n",
        "    plt.scatter(\n",
        "        cluster_points[\"Temperature(F)\"], \n",
        "        cluster_points[\"Humidity(%)\"],\n",
        "        label=f\"Cluster {cluster_id}\",\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Temperature(F)\")\n",
        "plt.ylabel(\"Humidity(%)\")\n",
        "plt.title(f\"K-Means Clustering (k={best_k})\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# another visualization with different features\n",
        "pandas_df2 = predictions_final.select(\n",
        "    \"Visibility(mi)\", \n",
        "    \"Wind_Speed(mph)\", \n",
        "    \"prediction\"\n",
        ").toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster_id in range(best_k):\n",
        "    cluster_points = pandas_df2[pandas_df2[\"prediction\"] == cluster_id]\n",
        "    plt.scatter(\n",
        "        cluster_points[\"Visibility(mi)\"], \n",
        "        cluster_points[\"Wind_Speed(mph)\"],\n",
        "        label=f\"Cluster {cluster_id}\",\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Visibility(mi)\")\n",
        "plt.ylabel(\"Wind_Speed(mph)\")\n",
        "plt.title(f\"K-Means Clustering (k={best_k})\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot elbow curve\n",
        "k_vals = [r['k'] for r in results]\n",
        "wcss_vals = [r['wcss'] for r in results]\n",
        "silhouette_vals = [r['silhouette'] for r in results]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(k_vals, wcss_vals, marker='o', linewidth=2)\n",
        "ax1.set_xlabel(\"Number of Clusters (k)\")\n",
        "ax1.set_ylabel(\"WCSS\")\n",
        "ax1.set_title(\"Elbow Method\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(k_vals, silhouette_vals, marker='o', linewidth=2, color='orange')\n",
        "ax2.set_xlabel(\"Number of Clusters (k)\")\n",
        "ax2.set_ylabel(\"Silhouette Score\")\n",
        "ax2.set_title(\"Silhouette Score by k\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare to dominics original results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON TO BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nDominic's Original Clustering (k=4, all 7 features):\")\n",
        "print(\"  Silhouette Score: 0.4713\")\n",
        "print(\"  WCSS: 186248.60\")\n",
        "print(\"  Features: Severity, Temperature, Humidity, Pressure, Visibility, Wind_Speed, Precipitation\")\n",
        "\n",
        "print(f\"\\nImproved Clustering (k={best_k}, top {len(all_features)} features):\")\n",
        "print(f\"  Silhouette Score: {final_silhouette:.4f}\")\n",
        "print(f\"  WCSS: {final_wcss:.2f}\")\n",
        "print(f\"  Features: {', '.join(all_features[:3])}... ({len(all_features)} total)\")\n",
        "\n",
        "improvement = ((final_silhouette - 0.4713) / 0.4713) * 100\n",
        "print(f\"\\nSilhouette improvement: {improvement:+.1f}%\")\n",
        "\n",
        "if final_silhouette > 0.4713:\n",
        "    print(\"Result: Improved clustering quality\")\n",
        "elif final_silhouette > 0.45:\n",
        "    print(\"Result: Similar clustering quality\")\n",
        "else:\n",
        "    print(\"Result: Lower clustering quality (may need more features)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show which features we used\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURES USED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "feature_importance = {\n",
        "    \"Temperature(F)\": 0.8496,\n",
        "    \"Wind_Chill(F)\": 0.5705,\n",
        "    \"Humidity(%)\": 0.5078,\n",
        "    \"Pressure(in)\": 0.3684,\n",
        "    \"Traffic_Signal\": 0.2706,\n",
        "    \"Visibility(mi)\": 0.2491,\n",
        "    \"Wind_Speed(mph)\": 0.2225,\n",
        "    \"Traffic_Calming\": 0.2145,\n",
        "    \"Precipitation(in)\": 0.1942,\n",
        "    \"Stop\": 0.1809\n",
        "}\n",
        "\n",
        "print(\"\\nTop 10 features (from Siddhant's analysis):\")\n",
        "for i, (feat, imp) in enumerate(feature_importance.items(), 1):\n",
        "    print(f\"{i:2d}. {feat:25s} {imp:.4f}\")\n",
        "\n",
        "print(\"\\nTemporal features (2):\")\n",
        "print(\"  - rush_hour\")\n",
        "print(\"  - is_weekend\")\n",
        "\n",
        "print(\"\\nWeather condition flags (4):\")\n",
        "print(\"  - is_clear\")\n",
        "print(\"  - is_rainy\")\n",
        "print(\"  - is_foggy\")\n",
        "print(\"  - is_snowy\")\n",
        "\n",
        "print(\"\\nRoad type flags (2):\")\n",
        "print(\"  - is_highway\")\n",
        "print(\"  - is_local\")\n",
        "\n",
        "print(\"\\nSpatial features (2):\")\n",
        "print(\"  - state_encoded\")\n",
        "print(\"  - high_accident_state\")\n",
        "\n",
        "print(f\"\\nTotal features: {len(all_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
